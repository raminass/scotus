{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from model import *\n",
    "notebook_login()\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "opinions_datatset = load_dataset('raminass/opinions')\n",
    "opinions_df = pd.DataFrame(opinions_datatset['train'])\n",
    "opinions_df = opinions_df[opinions_df.category!='per_curiam']\n",
    "del opinions_datatset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "author_name\n",
       "Justice Stevens      1181\n",
       "Justice Rehnquist     752\n",
       "Justice Brennan       732\n",
       "Justice Scalia        718\n",
       "Justice White         655\n",
       "Justice Blackmun      654\n",
       "Justice Marshall      611\n",
       "Justice Thomas        551\n",
       "Justice O'Connor      518\n",
       "Justice Kennedy       461\n",
       "Justice Powell        455\n",
       "Justice Burger        417\n",
       "Justice Breyer        416\n",
       "Justice Ginsburg      407\n",
       "Justice Stewart       328\n",
       "Justice Souter        291\n",
       "Justice Douglas       287\n",
       "Justice Alito         239\n",
       "Justice Sotomayor     175\n",
       "Justice Kagan         110\n",
       "Justice Roberts        94\n",
       "Justice Black          59\n",
       "Justice Harlan         50\n",
       "Justice Gorsuch        47\n",
       "Justice Kavanaugh      20\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opinions_df.author_name.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All Periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "opinion_train, opinion_test = train_test_split(opinions_df, random_state=1984, stratify=opinions_df['author_name'])\n",
    "id2label, label2id = get_labels_maping(opinions_df)\n",
    "paragraphs_test = split_data(opinion_test, id2label, label2id )\n",
    "paragraphs_train = split_data(opinion_train, id2label, label2id )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs_train_smpl = paragraphs_train.groupby('author_name',as_index = False,group_keys=False).apply(lambda s: s.sample(1500, replace=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59dc2edbec644c3a890b34b2c5b8fcaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/37500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e37274aee384108a170f22b2f7278a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/17992 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds_dict = {'train' : Dataset.from_pandas(paragraphs_train_smpl[[\"label\", \"text\"]], preserve_index=False),\n",
    "           'test' : Dataset.from_pandas(paragraphs_test[[\"label\", \"text\"]], preserve_index=False)}\n",
    "ds = DatasetDict(ds_dict)\n",
    "tokenized_court = tokenize_dataset(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = get_new_trainer(id2label, label2id, tokenized_court, epochs=20, model_name='1970-2020', batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "author_name\n",
       "Justice Brennan      575\n",
       "per_curiam           539\n",
       "Justice Marshall     463\n",
       "Justice White        462\n",
       "Justice Rehnquist    457\n",
       "Justice Stevens      454\n",
       "Justice Blackmun     439\n",
       "Justice Powell       421\n",
       "Justice Burger       417\n",
       "Justice Stewart      328\n",
       "Justice Douglas      287\n",
       "Justice O'Connor     140\n",
       "Justice Black         59\n",
       "Justice Harlan        50\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The Burger Court era, under the leadership of Warren E. Burger, lasted from June 23, 1969 to September 26, 1986; 13 justices served during this court.\n",
    "burger_df = opinions_df[opinions_df.date_filed.between('1969-06-23', '1986-09-26')]\n",
    "burger_df.author_name.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "author_name\n",
       "Justice Stevens      613\n",
       "Justice Scalia       497\n",
       "Justice O'Connor     376\n",
       "Justice Kennedy      298\n",
       "Justice Rehnquist    295\n",
       "Justice Thomas       244\n",
       "Justice Souter       239\n",
       "Justice Blackmun     215\n",
       "Justice White        192\n",
       "Justice Ginsburg     192\n",
       "per_curiam           188\n",
       "Justice Breyer       185\n",
       "Justice Brennan      157\n",
       "Justice Marshall     148\n",
       "Justice Powell        34\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The Rehnquist Court era, under the leadership of William Rehnquist, lasted from September 26, 1986 to September 3, 2005.\n",
    "rehnquist_df = opinions_df[opinions_df.date_filed.between('1986-09-26', '2005-09-03')]\n",
    "rehnquist_df.author_name.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "author_name\n",
       "Justice Thomas       307\n",
       "Justice Alito        239\n",
       "Justice Breyer       231\n",
       "Justice Scalia       221\n",
       "Justice Ginsburg     215\n",
       "Justice Sotomayor    175\n",
       "Justice Kennedy      163\n",
       "Justice Stevens      114\n",
       "Justice Kagan        110\n",
       "Justice Roberts       94\n",
       "Justice Souter        52\n",
       "Justice Gorsuch       47\n",
       "per_curiam            28\n",
       "Justice Kavanaugh     20\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  The Roberts Court era, under the leadership of John Roberts, began September 29, 2005, and is ongoing\n",
    "roberts_df = opinions_df[opinions_df.date_filed > '2005-09-29']\n",
    "roberts_df = roberts_df[~roberts_df.author_name.isin([\"Justice O'Connor\", 'Justice White'])]\n",
    "roberts_df.author_name.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8007, 19)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merge without adding new columns\n",
    "op = pd.merge(paragraphs_df, opinion_test, how='inner', on=['author_name', 'case_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(117, 19)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "op[op.author_name=='per_curiam'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs_test = split_data(opinion_test[opinion_test.category!='per_curiam'], id2label, label2id )\n",
    "paragraphs_train = split_data(opinion_train[opinion_train.category!='per_curiam'], id2label, label2id )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_dict = {'train' : Dataset.from_pandas(paragraphs_train[[\"label\", \"text\"]], preserve_index=False),\n",
    "           'test' : Dataset.from_pandas(paragraphs_test[[\"label\", \"text\"]], preserve_index=False)}\n",
    "ds = DatasetDict(ds_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at nlpaueb/legal-bert-small-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at nlpaueb/legal-bert-small-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits for Model 1: tensor([[0.2663, 0.3671]], grad_fn=<AddmmBackward0>)\n",
      "Logits for Model 2: tensor([[ 0.2011, -0.3665, -0.2145]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Specify the BERT model and tokenizer\n",
    "bert_model_name = \"nlpaueb/legal-bert-small-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(bert_model_name)\n",
    "\n",
    "# Create model_1\n",
    "model_1 = AutoModelForSequenceClassification.from_pretrained(bert_model_name, num_labels=2)\n",
    "\n",
    "# Example input text for fine-tuning\n",
    "input_text = \"Fine-tuning example sentence.\"\n",
    "labels = torch.tensor([1])  # Example labels (modify based on your task)\n",
    "\n",
    "# Tokenize the input text\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Forward pass through model_1\n",
    "output_1 = model_1(**inputs, labels=labels)\n",
    "\n",
    "# Create model_2 with the same BERT transformer as model_1\n",
    "model_2 = AutoModelForSequenceClassification.from_pretrained(bert_model_name, num_labels=3)\n",
    "\n",
    "# Initialize model_2 with the pre-trained weights from model_1\n",
    "model_2.bert = model_1.bert\n",
    "\n",
    "# Forward pass through model_2 for fine-tuning\n",
    "output_2 = model_2(**inputs, labels=labels)\n",
    "\n",
    "# Optionally, you can further fine-tune model_2 on your specific task using the provided training data\n",
    "\n",
    "# Display the logits for both models\n",
    "print(\"Logits for Model 1:\", output_1.logits)\n",
    "print(\"Logits for Model 2:\", output_2.logits)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scotus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
